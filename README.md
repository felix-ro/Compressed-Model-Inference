# Compressed Model Inference
This repository contains small experiments on how different optimization and compression techniques affect a model's latency. 

### Included Experiments: 
1. Compilation using XLA 
2. Depthwise Separable Convolutions
3. Pruning
4. Quantization