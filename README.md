# Compressed Model Inference
This repository contains small experiments of how different optimization and compression techniques affect a models latency. 

### Included Experiments: 
1. Compilation using XLA 
2. Depthwise Separable Convolutions
3. Pruning
4. Quantization